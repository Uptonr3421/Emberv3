# Emberv3 AI Assistant Framework

![Emberv3](https://img.shields.io/badge/Emberv3-Production%20Ready-brightgreen)
![Python](https://img.shields.io/badge/Python-3.13-blue)
![FastAPI](https://img.shields.io/badge/FastAPI-Async%20API-green)
![License](https://img.shields.io/badge/License-MIT-yellow)

> **A sophisticated, production-ready AI Assistant Framework featuring local LLM integration, real-time monitoring, and collaborative agent architecture.**

## ğŸŒŸ **Features**

### ğŸ¤– **AI & Model Management**
- **Local LLM Support**: Phi-3 model with GGUF format
- **Dual Model System**: Local + OpenAI API fallback
- **Real-time Generation**: Streaming text generation
- **Resource Monitoring**: CPU, memory, disk usage tracking
- **Performance Metrics**: Generation speed and token statistics

### ğŸ”§ **System Architecture**
- **Multi-Agent System**: Collaborative background agents
- **Real-time File Monitoring**: Automatic project state tracking
- **REST API Server**: FastAPI with auto-generated documentation
- **Background Processing**: Threaded component management
- **System Orchestration**: One-command startup and shutdown

### ğŸ¯ **Production Ready**
- **Comprehensive Testing**: 5-component test suite
- **Error Handling**: Graceful degradation and recovery
- **Logging System**: Structured logging with timestamps
- **Health Monitoring**: System status and diagnostic endpoints
- **Docker Support**: Containerized deployment ready

## ğŸš€ **Quick Start**

### **Prerequisites**
- Python 3.13+
- Linux/WSL environment
- 8GB+ RAM recommended
- 4GB+ disk space for models

### **Installation**
```bash
# Clone repository
git clone https://github.com/Uptonr3421/Emberv3.git
cd Emberv3

# Create virtual environment
python -m venv venv
source venv/bin/activate  # Linux/Mac
# or
venv\Scripts\activate  # Windows

# Install dependencies
pip install -r requirements_complete.txt

# Download AI model
python create_demo_model.py
```

### **Configuration**
Create/edit `.env` file:
```env
# Model Configuration
USE_LOCAL_MODEL=true
LOCAL_MODEL_PATH=models/phi-3-mini-4k-instruct-q4.gguf
MODEL_NAME=demo-model

# API Configuration
API_HOST=localhost
API_PORT=8000

# System Configuration
DEBUG=true
VERBOSE_LOGGING=true
MAX_TOKENS=2048
TEMPERATURE=0.7
```

### **Running the System**
```bash
# Run comprehensive tests
python quick_test.py

# Start the complete system
python start_system.py

# Or run components individually
python api_server.py          # REST API server
python model_manager_linux.py # Model manager only
```

## ğŸ—ï¸ **Architecture Overview**

```
Emberv3 AI Assistant Framework
â”œâ”€â”€ ğŸ§  Model Manager (model_manager_linux.py)
â”‚   â”œâ”€â”€ Local LLM Integration (llama-cpp-python)
â”‚   â”œâ”€â”€ OpenAI API Fallback
â”‚   â””â”€â”€ Performance Monitoring
â”œâ”€â”€ ğŸŒ API Server (api_server.py)
â”‚   â”œâ”€â”€ FastAPI REST Endpoints
â”‚   â”œâ”€â”€ Streaming Generation
â”‚   â””â”€â”€ Health Monitoring
â”œâ”€â”€ ğŸ‘¥ Agent System (agents/)
â”‚   â”œâ”€â”€ File Monitor (file_monitor.py)
â”‚   â””â”€â”€ Background Processing
â”œâ”€â”€ ğŸ”§ System Orchestration (start_system.py)
â”‚   â”œâ”€â”€ Multi-component Startup
â”‚   â”œâ”€â”€ Dependency Validation
â”‚   â””â”€â”€ Graceful Shutdown
â””â”€â”€ ğŸ§ª Testing Framework (quick_test.py)
    â”œâ”€â”€ Component Testing
    â”œâ”€â”€ Integration Testing
    â””â”€â”€ Performance Validation
```

## ğŸ“Š **API Documentation**

### **Generation Endpoints**
```bash
# Text generation
POST /generate
{
  "prompt": "Hello, how are you?",
  "max_tokens": 100,
  "temperature": 0.7
}

# Streaming generation
POST /generate/stream
{
  "prompt": "Tell me a story",
  "max_tokens": 500
}
```

### **System Endpoints**
```bash
# Health check
GET /health

# System status
GET /status

# Admin statistics
GET /admin/stats
```

### **Interactive Documentation**
- **Swagger UI**: `http://localhost:8000/docs`
- **ReDoc**: `http://localhost:8000/redoc`

## ğŸ” **Testing & Validation**

### **Test Suite Results**
```
ğŸ”¥ EMBER COMPONENT TESTS ğŸ”¥
âœ… Dependencies    - All packages installed
âœ… Environment     - Configuration loaded
âœ… API Imports     - FastAPI components working
âœ… Model Manager   - Local Phi-3 model operational
âœ… File Monitor    - 22 files tracked
```

### **Performance Metrics**
- **Generation Speed**: 1-2 tokens/second
- **Memory Usage**: ~4GB with Phi-3 model
- **API Response Time**: <100ms (excluding generation)
- **File Monitor**: Real-time change detection

## ğŸ› ï¸ **Development**

### **Project Structure**
```
Emberv3/
â”œâ”€â”€ agents/                    # Background agents
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ file_monitor.py       # File system monitoring
â”œâ”€â”€ models/                   # AI models directory
â”‚   â”œâ”€â”€ README.md
â”‚   â””â”€â”€ *.gguf               # Local models (gitignored)
â”œâ”€â”€ venv/                    # Virtual environment
â”œâ”€â”€ .cursorrules             # Hive mind agent rules
â”œâ”€â”€ .env                     # Environment configuration
â”œâ”€â”€ api_server.py            # FastAPI REST server
â”œâ”€â”€ model_manager_linux.py   # Linux model manager
â”œâ”€â”€ start_system.py          # System orchestration
â”œâ”€â”€ quick_test.py            # Testing framework
â””â”€â”€ requirements_complete.txt # Dependencies
```

### **Development Guidelines**
- **Main Branch Only**: No feature branches
- **Hive Mind Collaboration**: Multi-agent development
- **Test-Driven**: All changes tested before deployment
- **Documentation First**: Comprehensive docs for all features

### **Adding New Components**
1. Create component in appropriate directory
2. Add to `start_system.py` orchestration
3. Include in `quick_test.py` test suite
4. Update documentation and README

## ğŸ”§ **Configuration Reference**

### **Environment Variables**
| Variable | Description | Default |
|----------|-------------|---------|
| `USE_LOCAL_MODEL` | Enable local model | `true` |
| `LOCAL_MODEL_PATH` | Path to GGUF model | `models/phi-3-mini-4k-instruct-q4.gguf` |
| `API_HOST` | API server host | `localhost` |
| `API_PORT` | API server port | `8000` |
| `DEBUG` | Enable debug logging | `false` |
| `MAX_TOKENS` | Maximum generation tokens | `2048` |
| `TEMPERATURE` | Generation temperature | `0.7` |

### **Model Configuration**
- **Supported Formats**: GGUF, GGML
- **Recommended Models**: Phi-3, Llama-2, Mistral
- **Memory Requirements**: 4GB+ for 7B parameter models
- **Performance**: CPU-optimized inference

## ğŸš¨ **Troubleshooting**

### **Common Issues**
1. **Model Not Found**: Run `python create_demo_model.py`
2. **Memory Issues**: Reduce model size or increase system RAM
3. **Port Conflicts**: Change `API_PORT` in `.env`
4. **Permission Errors**: Ensure proper file permissions

### **Debug Mode**
```bash
# Enable verbose logging
export DEBUG=true
export VERBOSE_LOGGING=true

# Run with debug output
python start_system.py
```

### **Log Locations**
- **System Logs**: Console output with timestamps
- **Error Logs**: Captured in application logs
- **Performance Metrics**: Available via `/admin/stats`

## ğŸ¤ **Contributing**

### **Development Workflow**
1. Fork repository
2. Work on main branch (no feature branches)
3. Run test suite: `python quick_test.py`
4. Submit pull request

### **Code Standards**
- **Python**: PEP 8 compliance
- **Documentation**: Comprehensive docstrings
- **Testing**: All new features tested
- **Error Handling**: Graceful failure modes

## ğŸ“„ **License**

This project is licensed under the MIT License - see the LICENSE file for details.

## ğŸ”— **Links**

- **Documentation**: [README.md](README.md)
- **API Docs**: http://localhost:8000/docs
- **GitHub**: https://github.com/Uptonr3421/Emberv3
- **Issues**: https://github.com/Uptonr3421/Emberv3/issues

---

**Built with â¤ï¸ by the Emberv3 Development Team**

*Powered by Phi-3, FastAPI, and cutting-edge AI technology*